{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e13d3f2dd817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msubprocess\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.misc, os, csv, random, math\n",
    "from subprocess import check_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AC\n",
    "#trainImagePath='C:/Users/ANTHO/Desktop/TestTensorFlow/KAGGLE/Dataset/imgpp/'\n",
    "#labelsPath = 'C:/Users/ANTHO/Desktop/TestTensorFlow/KAGGLE/Dataset/GT/'\n",
    "\n",
    "# VSP\n",
    "#trainImagePath='/Users/valurpalmarsson/Documents/DeepLearning/imgpp/'\n",
    "#labelsPath = '/Users/valurpalmarsson/Documents/DeepLearning/GT/'\n",
    "\n",
    "trainImagePath='./Dataset/imgpp/'\n",
    "labelsPath = './Dataset/GT/'\n",
    "\n",
    "mainPath='./Measure'\n",
    "\n",
    "epochs=5\n",
    "batchSize=10\n",
    "learningRate=1e-4\n",
    "numImg = 200 # Number of images to Load\n",
    "shuffleData = False # Shuffle data\n",
    "\n",
    "# Size of each subset as precentage of entire dataset\n",
    "trainSize=0.5\n",
    "valSize=0.25\n",
    "testSize=0.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(os.path.exists(path))\n",
    "\n",
    "path = mainPath\n",
    "i=0\n",
    "while os.path.exists(path):\n",
    "    path = mainPath+str(i)\n",
    "    i=i+1\n",
    "    print(path)\n",
    "\n",
    "os.makedirs(path)\n",
    "os.makedirs(path+'/Tensorboard')\n",
    "os.makedirs(path+'/SaveModel')\n",
    "os.makedirs(path+'/Results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the images & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract all the file in the folder\n",
    "fileNames = os.listdir(trainImagePath)\n",
    "\n",
    "# select a subset of files\n",
    "fileNames = fileNames[:numImg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract all the image flatten and write them in an array\n",
    "imgs=[] # Init image list\n",
    "labels=[] # \n",
    "for fileName in fileNames: \n",
    "    \n",
    "    # Load images\n",
    "    img = scipy.misc.imread(trainImagePath+fileName, False,'RGB')     \n",
    "    img = img.ravel()#Flatten the image\n",
    "    imgs.append(img)\n",
    "    \n",
    "    # Load labels\n",
    "    labelName = fileName.partition('pp')[0]+'GT.csv'\n",
    "    with open(labelsPath+labelName, 'r') as f:#Read the file\n",
    "        reader = csv.reader(f)\n",
    "        label= np.asarray(list(reader), dtype=float)#Extract the value in an array\n",
    "        label = label.ravel()#flatten\n",
    "        labels.append(label)#Add to the list\n",
    "        \n",
    "imgs=np.matrix(imgs, dtype=float)/255\n",
    "labels = np.matrix(labels, dtype=float)#Convert to a matrix\n",
    "    \n",
    "#Size(nbImage, nbPixel)\n",
    "print(imgs.shape)\n",
    "print(len(imgs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select size of train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates train, validate and test subsets from the loaded images\n",
    "# Can shuffle the images by using the boolean input argument shuffle\n",
    "def createDatasets(imgs,labels,trainSize,valSize,testSize,shuffle):\n",
    "\n",
    "    # Get number of images\n",
    "    numImg = len(imgs) \n",
    "    \n",
    "    # Indexes used to split the entire dataset into subsets\n",
    "    indx1 = int(numImg*trainSize)\n",
    "    indx2 = int(numImg*(trainSize+valSize))\n",
    "    \n",
    "    if shuffle:\n",
    "        # Shuffle images to a random order, (shuffles imgs and labels in the same random order)\n",
    "        combined = list(zip(imgs, labels))\n",
    "        random.shuffle(combined)\n",
    "        imgs[:], labels[:] = zip(*combined)\n",
    "\n",
    "    # Training subset\n",
    "    imgsTrain = imgs[0:indx1]\n",
    "    labelsHotTrain = labels[0:indx1]\n",
    "\n",
    "    # Validation subset\n",
    "    imgsValidate = imgs[indx1:indx2]\n",
    "    labelsHotValidate = labels[indx1:indx2]\n",
    "\n",
    "    # Test subset\n",
    "    imgsTest = imgs[indx2:]\n",
    "    labelsHotTest = labels[indx2:]\n",
    "    \n",
    "    # Return the subsets as well as the corresponding ground-truth\n",
    "    return imgsTrain,labelsHotTrain,imgsValidate,labelsHotValidate,imgsTest,labelsHotTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS ##\n",
    "tensorBoard=True      #Save a summary?\n",
    "saveVariable=False     #Save the variabe?\n",
    "#retrieveSavedVariable=True #Retrieve saved variable?\n",
    "\n",
    "# Split dataset to subsets\n",
    "imgsTrain,labelsHotTrain,imgsValidate,labelsHotValidate,imgsTest,labelsHotTest = createDatasets(imgs,labels,trainSize,valSize,testSize,shuffleData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(imgs[0].shape)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    y = tf.reshape(labelsHotTest,[-1,43,43,1])\n",
    "    \n",
    "    y_pre = sess.run(prediction, feed_dict={xs: imgsTest})\n",
    "    y_pre = tf.cast(y_pre, tf.float32)#Force float32\n",
    "    \n",
    "    y_label = tf.reshape(labelsHotTest, [-1, 43, 43, 1])\n",
    "    y_label = tf.cast(y_label, tf.float32)\n",
    "    \n",
    "    summa = tf.reduce_sum(y,0)\n",
    "    summavec = tf.reshape(summa,[-1])\n",
    "    \n",
    "    #print(y_pre)\n",
    "    print(y_label)\n",
    "    print(y_pre)\n",
    "    #print(tf.metrics.mean_absolute_error(tf.reduce_sum(y_pre,[1,2]),tf.reduce_sum(y_label,[1,2])))\n",
    "    #print(tf.reduce_mean(tf.square(tf.reduce_max(tf.subtract(y_pre,y_label),[1,2]))))\n",
    "    #print(tf.reduce_sum(y_label,[1,2]),tf.reduce_sum(y_pre,[1,2]))\n",
    "    diff = tf.subtract(tf.reduce_sum(y_label,[1,2]),tf.reduce_sum(y_pre,[1,2]))\n",
    "    print(tf.sqrt(tf.reduce_mean(tf.square(diff))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function\n",
    "\n",
    "def evaluateNetwork(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs}) # Forward pass using v_xs as input to network\n",
    "    y_pre = tf.cast(y_pre, tf.float32) # Force float32 \n",
    "    result = tf.reduce_sum(y_pre) # Integrate over density map to get estimated sea lion count\n",
    "    return result\n",
    "\n",
    "# Calculate mean absolute error (MAE) and mean square error (MSE) for evaluation\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_label = tf.reshape(v_ys, [-1, 43, 43, 1])\n",
    "    y_label = tf.cast(y_label, tf.float32)\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs})\n",
    "    y_pre = tf.cast(y_pre, tf.float32)#Force float32\n",
    "    #correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    diff = tf.subtract(tf.reduce_sum(y_label,[1,2]),tf.reduce_sum(y_pre,[1,2]))\n",
    "    accuracyMAE = tf.cast(tf.reduce_mean(tf.abs(diff)),tf.float32)\n",
    "    accuracyMSE = tf.cast(tf.sqrt(tf.reduce_mean(tf.square(diff))),tf.float32)\n",
    "    MAE = sess.run(accuracyMAE, feed_dict={xs: v_xs, ys: v_ys})\n",
    "    MSE = sess.run(accuracyMSE, feed_dict={xs: v_xs, ys: v_ys})\n",
    "    \n",
    "    return MAE,MSE\n",
    "\n",
    "def weight_variable(shape, nameIn):\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=nameIn)\n",
    "\n",
    "def bias_variable(shape, nameIn):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=nameIn)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    # Must have strides[0] = strides[3] = 1\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def max_pool_3x3(x):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    return tf.nn.max_pool(x, ksize=[1,3,3,1], strides=[1,3,3,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Architecture\n",
    "\n",
    "## Input layer ##\n",
    "with tf.name_scope(\"Input\") as scope:\n",
    "    # define placeholder for inputs to network\n",
    "    xs = tf.placeholder(tf.float32, [None, imgs.shape[1]])   # 256x256x3\n",
    "    ys = tf.placeholder(tf.float32, [None, labels.shape[1]]) #43x43 \n",
    "    x_image = tf.reshape(xs, [-1, 256, 256, 3])#[batch, in_depth, in_height, in_width, in_channels].\n",
    "    y_label = tf.reshape(ys, [-1, 43, 43, 1])\n",
    "    #print(x_image.shape)  # [n_samples, 28,28,1]\n",
    "    \n",
    "## conv1 layer ##\n",
    "## maxpooling 2x2 ##\n",
    "#20x (7x7)\n",
    "patch=7\n",
    "sizeIn=3\n",
    "sizeOut=20\n",
    "with tf.name_scope(\"Conv1\") as scope:\n",
    "    W_conv1 = weight_variable([patch,patch, sizeIn,sizeOut], \"W_conv1\")\n",
    "    b_conv1 = bias_variable([sizeOut], \"b_conv1\")\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1) #Maxpooling 2x2, output size= inout size/2\n",
    "    # Add summary ops to collect data\n",
    "    w_h_conv1 = tf.summary.histogram(\"weightsConv1\", W_conv1)\n",
    "    b_h_conv1 = tf.summary.histogram(\"biasesConv1\", b_conv1)\n",
    "    \n",
    "## conv2 layer ##\n",
    "## maxpooling 3x3 ##\n",
    "#40x (5x5)\n",
    "patch=5\n",
    "sizeIn=sizeOut #From previous ConvNet\n",
    "sizeOut=40\n",
    "with tf.name_scope(\"Conv2\") as scope:\n",
    "    W_conv2 = weight_variable([patch,patch, sizeIn,sizeOut],\"W_conv2\")\n",
    "    b_conv2 = bias_variable([sizeOut],\"b_conv2\")\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_3x3(h_conv2) #Maxpooling 3x3, output size= inout size/3\n",
    "    # Add summary ops to collect data\n",
    "    w_h_conv2 = tf.summary.histogram(\"weightsConv2\", W_conv2)\n",
    "    b_h_conv2 = tf.summary.histogram(\"biasesConv2\", b_conv2)\n",
    "    \n",
    "## conv3 layer ##\n",
    "#20x (5x5)\n",
    "patch=5\n",
    "sizeIn=sizeOut #From previous ConvNet\n",
    "sizeOut=20\n",
    "with tf.name_scope(\"Conv3\") as scope:\n",
    "    W_conv3 = weight_variable([patch,patch, sizeIn,sizeOut],\"W_conv3\")\n",
    "    b_conv3 = bias_variable([sizeOut],\"b_conv3\")\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "    # Add summary ops to collect data\n",
    "    w_h_conv3 = tf.summary.histogram(\"weightsConv3\", W_conv3)\n",
    "    b_h_conv3 = tf.summary.histogram(\"biasesConv3\", b_conv3)\n",
    "    \n",
    "## conv4 layer ##\n",
    "#10x (5x5)\n",
    "patch=5\n",
    "sizeIn=sizeOut #From previous ConvNet\n",
    "sizeOut=10\n",
    "with tf.name_scope(\"Conv4\") as scope:\n",
    "    W_conv4 = weight_variable([patch,patch, sizeIn,sizeOut],\"W_conv4\")\n",
    "    b_conv4 = bias_variable([sizeOut],\"b_conv4\")\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4) + b_conv4)\n",
    "    # Add summary ops to collect data\n",
    "    w_h_conv4 = tf.summary.histogram(\"weightsConv4\", W_conv4)\n",
    "    b_h_conv4 = tf.summary.histogram(\"biasesConv4\", b_conv4)\n",
    "    \n",
    "## conv5 layer ##\n",
    "#1x (1x1)\n",
    "patch=1\n",
    "sizeIn=sizeOut #From previous ConvNet\n",
    "sizeOut=1\n",
    "with tf.name_scope(\"Conv5\") as scope:\n",
    "    W_conv5 = weight_variable([patch,patch, sizeIn,sizeOut], \"W_conv5\")\n",
    "    b_conv5 = bias_variable([sizeOut], \"b_conv5\")\n",
    "    h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5) + b_conv5)\n",
    "    #h_conv5 = conv2d(h_conv4, W_conv5) + b_conv5\n",
    "    # Add summary ops to collect data\n",
    "    w_h_conv5 = tf.summary.histogram(\"weightsConv5\", W_conv5)\n",
    "    b_h_conv5 = tf.summary.histogram(\"biasesConv5\", b_conv5)\n",
    "    \n",
    "   \n",
    " ## Prediction ##\n",
    "with tf.name_scope(\"prediction\") as scope:\n",
    "    prediction = h_conv5\n",
    "    #img_prediction = tf.summary.image(\"densitymap\", prediction)\n",
    "\n",
    "\n",
    "## Loss function ##\n",
    "# the error between prediction and real data\n",
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))# loss\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    loss = tf.reduce_mean(tf.square(tf.reduce_max(tf.subtract(prediction,y_label),[1,2])))\n",
    "    #cross_entropy = tf.reshape(tf.reduce_sum(tf.square((tf.subtract(prediction, y_label))),0),[-1])#Euclidean distance\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    train_step = tf.train.AdamOptimizer(learningRate).minimize(loss)\n",
    "    #train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Bla,',imgsTest[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## INITIALISATION ##\n",
    "if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "    init = tf.initialize_all_variables()\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    \n",
    "## TENSOR BOARD ##\n",
    "if tensorBoard:\n",
    "    # Merge all summaries into a single operator\n",
    "    merged_summary_op = tf.summary.merge_all() \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    ### SAVE DATA ###\n",
    "    if saveVariable:\n",
    "        saver = tf.train.Saver()\n",
    " \n",
    "    ### TENSORBOARD ###\n",
    "    if tensorBoard:\n",
    "        # Folder where the data are saved\n",
    "        summary_writer = tf.summary.FileWriter(path+'/TensorBoard/', sess.graph)\n",
    "\n",
    "\n",
    "    if int(len(imgs)) < batchSize:#If there are less data than the batch size\n",
    "        nbBatch=1\n",
    "    else:\n",
    "        nbBatch=int((len(imgsTrain))/batchSize)\n",
    "    \n",
    "    # Init variables \n",
    "    trainMAE =  [0 for x in range(epochs)]\n",
    "    validateMAE =  [0 for x in range(epochs)]\n",
    "    trainMSE =  [0 for x in range(epochs)]\n",
    "    validateMSE =  [0 for x in range(epochs)]\n",
    "    \n",
    "    #print(compute_accuracy(imgsTest,labelsHotTest))\n",
    "\n",
    "    #print(compute_accuracy(imgsTest, labelsHotTest))\n",
    "    for epoch in range(epochs):#Go through all the epochs\n",
    "        for batchNum in range(nbBatch):#Go through all the batches\n",
    "            #print(batchNum)\n",
    "            if batchSize==1:\n",
    "                batch_xs = imgsTrain[batchNum*batchSize]\n",
    "                batch_ys = labelsHotTrain[batchNum*batchSize]\n",
    "            else:\n",
    "                batch_xs = imgsTrain[batchNum*batchSize:(batchNum*batchSize+(batchSize))]\n",
    "                batch_ys = labelsHotTrain[batchNum*batchSize:(batchNum*batchSize+(batchSize))]\n",
    "                #print(len(batch_xs))\n",
    "            \n",
    "            sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys})\n",
    "        \n",
    "        print('Epoch: ',epoch+1)\n",
    "        \n",
    "        trainMAE[epoch],trainMSE[epoch] = compute_accuracy(imgsTrain, labelsHotTrain)\n",
    "        validateMAE[epoch],validateMSE[epoch] = compute_accuracy(imgsValidate, labelsHotValidate)\n",
    "        \n",
    "        print('Training MAE and MSE: ',trainMAE[epoch],trainMSE[epoch])\n",
    "        print('Validation MAE and MSE: ',validateMAE[epoch],validateMSE[epoch])\n",
    "        \n",
    "        ### TENSORBOARD ###\n",
    "        if tensorBoard:\n",
    "            summary_str = sess.run(merged_summary_op, feed_dict={xs: batch_xs, ys: batch_ys})\n",
    "            summary_writer.add_summary(summary_str, epoch)\n",
    "        ### SAVE DATA ###\n",
    "        if epoch%1==0:\n",
    "            if saveVariable:\n",
    "                saver.save(sess, path+'/SaveModel/cnnSave', global_step=epoch)\n",
    "    testMAE, testMSE = compute_accuracy(imgsTest, labelsHotTest)       \n",
    "    print('Test MAE and MSE: ',testMAE,testMSE)\n",
    "    \n",
    "    #Save the MAE and MSE of each epoch\n",
    "    np.savetxt(path+\"/Results/EpochMAEMSE.csv\", np.transpose([trainMAE, trainMSE, validateMAE, validateMSE]), delimiter=\",\")\n",
    "    np.savetxt(path+\"/Results/TESTMAEMSE.csv\", np.transpose([testMAE, testMSE]), delimiter=\",\")\n",
    "    np.savetxt(path+\"/Results/PARAM.csv\", np.transpose([epoch, batchSize, learningRate, numImg, shuffleData, trainSize, valSize, testSize]), delimiter=\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot MAE and MSE during training\n",
    "\n",
    "plt.subplot(121) # MAE\n",
    "plt.plot(np.linspace(1,epochs,epochs),trainMAE,'b',label='train')\n",
    "plt.plot(np.linspace(1,epochs,epochs),validateMAE,'r',label='validation')\n",
    "plt.xlabel('Training time [epoch]')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend(['train','validation'])\n",
    "plt.title('Mean Absolute Error')\n",
    "\n",
    "plt.subplot(122) # MSE\n",
    "plt.plot(np.linspace(1,epochs,epochs),trainMSE,'b',label='train')\n",
    "plt.plot(np.linspace(1,epochs,epochs),validateMSE,'r',label='validation')\n",
    "plt.xlabel('Training time [epoch]')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(['train','validation'])\n",
    "plt.title('Mean Square Error')\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+'/Results/graphs.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
