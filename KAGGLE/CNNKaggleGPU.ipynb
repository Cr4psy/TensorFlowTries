{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.misc, os, csv, random, math\n",
    "from subprocess import check_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AC\n",
    "#trainImagePath='C:/Users/ANTHO/Desktop/TestTensorFlow/KAGGLE/Dataset/imgpp/'\n",
    "#labelsPath = 'C:/Users/ANTHO/Desktop/TestTensorFlow/KAGGLE/Dataset/GT/'\n",
    "\n",
    "# VSP\n",
    "#trainImagePath='/Users/valurpalmarsson/Documents/DeepLearning/imgpp/'\n",
    "#labelsPath = '/Users/valurpalmarsson/Documents/DeepLearning/GT/'\n",
    "\n",
    "trainImagePath='./Dataset/imgpp/'\n",
    "labelsPath = './Dataset/GT/'\n",
    "\n",
    "mainPath='./Measure'\n",
    "\n",
    "epochs=6\n",
    "batchSize=10\n",
    "learningRate=1e-4\n",
    "numImg = 200 # Number of images to Load\n",
    "shuffleData = False # Shuffle data\n",
    "\n",
    "# Size of each subset as precentage of entire dataset\n",
    "trainSize=0.6\n",
    "valSize=0.2\n",
    "testSize=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new folder for the measures ###\n",
    "path = mainPath\n",
    "i=0\n",
    "while os.path.exists(path):\n",
    "    path = mainPath+str(i)\n",
    "    i=i+1\n",
    "    print(path)\n",
    "\n",
    "os.makedirs(path)\n",
    "os.makedirs(path+'/TensorBoard')\n",
    "os.makedirs(path+'/SaveModel')\n",
    "os.makedirs(path+'/Results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the images & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def importImages(nbImgStart, nbImgEnd):\n",
    "    #Extract all the file in the folder\n",
    "    fileNames = os.listdir(trainImagePath)\n",
    "    # select a subset of files\n",
    "    fileNames = fileNames[nbImgStart:nbImgEnd]\n",
    "    #Extract all the image flatten and write them in an array\n",
    "    \n",
    "    imgs=[] # Init image list\n",
    "    labels=[] # \n",
    "    for fileName in fileNames: \n",
    "        # Load images\n",
    "        img = scipy.misc.imread(trainImagePath+fileName, False,'RGB')     \n",
    "        img = img.ravel()#Flatten the image\n",
    "        imgs.append(img)\n",
    "\n",
    "        # Load labels\n",
    "        labelName = fileName.partition('pp')[0]+'GT.csv'\n",
    "        with open(labelsPath+labelName, 'r') as f:#Read the file\n",
    "            reader = csv.reader(f)\n",
    "            label= np.asarray(list(reader), dtype=float)#Extract the value in an array\n",
    "            label = label.ravel()#flatten\n",
    "            labels.append(label)#Add to the list\n",
    "\n",
    "    imgs=np.matrix(imgs, dtype=float)/255\n",
    "    labels = np.matrix(labels, dtype=float)#Convert to a matrix\n",
    "    \n",
    "    \n",
    "    # Shuffle images to a random order, (shuffles imgs and labels in the same random order)\n",
    "    #combined = list(zip(imgs, labels))\n",
    "    #random.shuffle(combined)\n",
    "    #imgs[:], labels[:] = zip(*combined)\n",
    "\n",
    "    #Size(nbImage, nbPixel)\n",
    "    #print(imgs.shape)\n",
    "    #print(len(imgs))\n",
    "    \n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS ##\n",
    "tensorBoard=True      #Save a summary?\n",
    "saveVariable=False     #Save the variabe?\n",
    "#retrieveSavedVariable=True #Retrieve saved variable?\n",
    "\n",
    "# Split dataset to subsets\n",
    "#imgsTrain,labelsHotTrain,imgsValidate,labelsHotValidate,imgsTest,labelsHotTest = createDatasets(imgs,labels,trainSize,valSize,testSize,shuffleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function\n",
    "\n",
    "def evaluateNetwork(v_xs):\n",
    "    global prediction\n",
    "    y_pre = sess.run( tf.cast(prediction, tf.float32), feed_dict={xs: v_xs}) # Forward pass using v_xs as input to network\n",
    "    result = np.matrix(y_pre, dtype=float)\n",
    "    return result\n",
    "\n",
    "# Calculate mean absolute error (MAE) and mean square error (MSE) for evaluation\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_label = tf.reshape(v_ys, [-1, 43, 43, 1])\n",
    "    y_label = tf.cast(y_label, tf.float32)\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs})\n",
    "    y_pre = tf.cast(y_pre, tf.float32)#Force float32\n",
    "    #correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    diff = tf.subtract(tf.reduce_sum(y_label,[1,2]),tf.reduce_sum(y_pre,[1,2]))\n",
    "    accuracyMAE = tf.cast(tf.reduce_mean(tf.abs(diff)),tf.float32)\n",
    "    accuracyMSE = tf.cast(tf.sqrt(tf.reduce_mean(tf.square(diff))),tf.float32)\n",
    "    MAE = sess.run(accuracyMAE, feed_dict={xs: v_xs, ys: v_ys})\n",
    "    MSE = sess.run(accuracyMSE, feed_dict={xs: v_xs, ys: v_ys})\n",
    "    \n",
    "    return MAE,MSE\n",
    "\n",
    "def weight_variable(shape, nameIn):\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=0.1)   \n",
    "    #initial = tf.random_normal(shape=shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=nameIn)\n",
    "\n",
    "def bias_variable(shape, nameIn):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=nameIn)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    # Must have strides[0] = strides[3] = 1\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def max_pool_3x3(x):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    return tf.nn.max_pool(x, ksize=[1,3,3,1], strides=[1,3,3,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Architecture\n",
    "\n",
    "## Input layer ##\n",
    "with tf.name_scope(\"Input\") as scope:\n",
    "    # define placeholder for inputs to network\n",
    "    xs = tf.placeholder(tf.float32, [None, 256*256*3])   # 256x256x3\n",
    "    ys = tf.placeholder(tf.float32, [None,43*43]) #43x43 \n",
    "    x_image = tf.reshape(xs, [-1, 256, 256, 3])#[batch, in_depth, in_height, in_width, in_channels].\n",
    "    y_label = tf.reshape(ys, [-1, 43, 43, 1])\n",
    "    #print(x_image.shape)  # [n_samples, 28,28,1]\n",
    "    \n",
    "## conv1 layer ##\n",
    "## maxpooling 2x2 ##\n",
    "#20x (7x7)\n",
    "patch=7\n",
    "sizeIn=3\n",
    "sizeOut=20\n",
    "with tf.name_scope(\"Conv1\") as scope:\n",
    "    W_conv1 = weight_variable([patch,patch, sizeIn,sizeOut], \"W_conv1\")\n",
    "    b_conv1 = bias_variable([sizeOut], \"b_conv1\")\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1) #Maxpooling 2x2, output size= inout size/2\n",
    "    # Add summary ops to collect data\n",
    "    #w_h_conv1 = tf.summary.histogram(\"weightsConv1\", W_conv1)\n",
    "    #b_h_conv1 = tf.summary.histogram(\"biasesConv1\", b_conv1)\n",
    "    \n",
    "## conv2 layer ##\n",
    "## maxpooling 3x3 ##\n",
    "#40x (5x5)\n",
    "patch=5\n",
    "sizeIn=sizeOut #From previous ConvNet\n",
    "sizeOut=40\n",
    "with tf.name_scope(\"Conv2\") as scope:\n",
    "    W_conv2 = weight_variable([patch,patch, sizeIn,sizeOut],\"W_conv2\")\n",
    "    b_conv2 = bias_variable([sizeOut],\"b_conv2\")\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_3x3(h_conv2) #Maxpooling 3x3, output size= inout size/3\n",
    "    # Add summary ops to collect data\n",
    "    #w_h_conv2 = tf.summary.histogram(\"weightsConv2\", W_conv2)\n",
    "    #b_h_conv2 = tf.summary.histogram(\"biasesConv2\", b_conv2)\n",
    "    \n",
    "## conv3 layer ##\n",
    "#20x (5x5)\n",
    "patch=5\n",
    "sizeIn=sizeOut #From previous ConvNet\n",
    "sizeOut=20\n",
    "with tf.name_scope(\"Conv3\") as scope:\n",
    "    W_conv3 = weight_variable([patch,patch, sizeIn,sizeOut],\"W_conv3\")\n",
    "    b_conv3 = bias_variable([sizeOut],\"b_conv3\")\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "    # Add summary ops to collect data\n",
    "    #w_h_conv3 = tf.summary.histogram(\"weightsConv3\", W_conv3)\n",
    "    #b_h_conv3 = tf.summary.histogram(\"biasesConv3\", b_conv3)\n",
    "    \n",
    "## conv4 layer ##\n",
    "#10x (5x5)\n",
    "patch=5\n",
    "sizeIn=sizeOut #From previous ConvNet\n",
    "sizeOut=10\n",
    "with tf.name_scope(\"Conv4\") as scope:\n",
    "    W_conv4 = weight_variable([patch,patch, sizeIn,sizeOut],\"W_conv4\")\n",
    "    b_conv4 = bias_variable([sizeOut],\"b_conv4\")\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4) + b_conv4)\n",
    "    # Add summary ops to collect data\n",
    "    #w_h_conv4 = tf.summary.histogram(\"weightsConv4\", W_conv4)\n",
    "    #b_h_conv4 = tf.summary.histogram(\"biasesConv4\", b_conv4)\n",
    "    \n",
    "## conv5 layer ##\n",
    "#1x (1x1)\n",
    "patch=1\n",
    "sizeIn=sizeOut #From previous ConvNet\n",
    "sizeOut=1\n",
    "with tf.name_scope(\"Conv5\") as scope:\n",
    "    W_conv5 = weight_variable([patch,patch, sizeIn,sizeOut], \"W_conv5\")\n",
    "    b_conv5 = bias_variable([sizeOut], \"b_conv5\")\n",
    "    h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5) + b_conv5)\n",
    "    #h_conv5 = conv2d(h_conv4, W_conv5) + b_conv5\n",
    "    # Add summary ops to collect data\n",
    "    #w_h_conv5 = tf.summary.histogram(\"weightsConv5\", W_conv5)\n",
    "    #b_h_conv5 = tf.summary.histogram(\"biasesConv5\", b_conv5)\n",
    "    \n",
    "   \n",
    " ## Prediction ##\n",
    "with tf.name_scope(\"prediction\") as scope:\n",
    "    prediction = h_conv5\n",
    "    #img_prediction = tf.summary.image(\"densitymap\", prediction)\n",
    "\n",
    "\n",
    "## Loss function ##\n",
    "# the error between prediction and real data\n",
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))# loss\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    loss = tf.reduce_mean(tf.square(tf.reduce_max(tf.abs(tf.subtract(prediction,y_label)),[1,2])))\n",
    "    #cross_entropy = tf.reshape(tf.reduce_sum(tf.square((tf.subtract(prediction, y_label))),0),[-1])#Euclidean distance\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    train_step = tf.train.AdamOptimizer(learningRate).minimize(loss)\n",
    "    #train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Measure0\n",
      "./Measure1\n",
      "./Measure2\n",
      "./Measure3\n",
      "./Measure4\n",
      "./Measure5\n",
      "./Measure6\n",
      "./Measure7\n",
      "./Measure8\n",
      "./Measure9\n",
      "./Measure10\n",
      "./Measure11\n",
      "./Measure12\n",
      "./Measure13\n",
      "./Measure14\n",
      "./Measure15\n",
      "./Measure16\n",
      "./Measure17\n",
      "./Measure18\n",
      "./Measure19\n",
      "./Measure20\n",
      "./Measure21\n",
      "./Measure22\n",
      "./Measure23\n",
      "./Measure24\n",
      "./Measure25\n",
      "./Measure26\n",
      "./Measure27\n",
      "./Measure28\n",
      "./Measure29\n",
      "./Measure30\n",
      "./Measure31\n",
      "./Measure32\n",
      "./Measure33\n",
      "./Measure34\n",
      "./Measure35\n",
      "./Measure36\n",
      "./Measure37\n",
      "./Measure38\n",
      "./Measure39\n",
      "./Measure40\n",
      "./Measure41\n",
      "./Measure42\n",
      "./Measure43\n",
      "./Measure44\n",
      "./Measure45\n",
      "./Measure46\n",
      "./Measure47\n",
      "./Measure48\n",
      "./Measure49\n",
      "./Measure50\n",
      "./Measure51\n",
      "./Measure52\n",
      "./Measure53\n",
      "./Measure54\n",
      "./Measure55\n",
      "./Measure56\n",
      "./Measure57\n",
      "./Measure58\n",
      "./Measure59\n",
      "./Measure60\n",
      "./Measure61\n",
      "./Measure62\n",
      "./Measure63\n",
      "./Measure64\n",
      "./Measure65\n",
      "./Measure66\n",
      "./Measure67\n",
      "0.0001\n",
      "Epoch:  1\n",
      "Training MAE and MSE:  185.270187378 186.534823476\n",
      "Validate MAE and MSE:  194.305244446 194.494693197\n",
      "Epoch:  2\n",
      "Training MAE and MSE:  202.195426941 203.398468837\n",
      "Validate MAE and MSE:  214.101593018 214.255186112\n",
      "Epoch:  3\n",
      "Training MAE and MSE:  241.215685527 243.044518083\n",
      "Validate MAE and MSE:  277.204521179 277.30423838\n",
      "Epoch:  4\n",
      "Training MAE and MSE:  359.301612854 363.776927836\n",
      "Validate MAE and MSE:  441.190597534 442.231520123\n",
      "Epoch:  5\n",
      "Training MAE and MSE:  526.204940796 528.499470576\n",
      "Validate MAE and MSE:  569.254058838 571.630070921\n",
      "Epoch:  6\n",
      "Training MAE and MSE:  563.822494507 565.383614104\n",
      "Validate MAE and MSE:  546.244918823 548.165444692\n",
      "Test MAE and MSE:  566.77243042 567.32384227\n",
      "./Measure0\n",
      "./Measure1\n",
      "./Measure2\n",
      "./Measure3\n",
      "./Measure4\n",
      "./Measure5\n",
      "./Measure6\n",
      "./Measure7\n",
      "./Measure8\n",
      "./Measure9\n",
      "./Measure10\n",
      "./Measure11\n",
      "./Measure12\n",
      "./Measure13\n",
      "./Measure14\n",
      "./Measure15\n",
      "./Measure16\n",
      "./Measure17\n",
      "./Measure18\n",
      "./Measure19\n",
      "./Measure20\n",
      "./Measure21\n",
      "./Measure22\n",
      "./Measure23\n",
      "./Measure24\n",
      "./Measure25\n",
      "./Measure26\n",
      "./Measure27\n",
      "./Measure28\n",
      "./Measure29\n",
      "./Measure30\n",
      "./Measure31\n",
      "./Measure32\n",
      "./Measure33\n",
      "./Measure34\n",
      "./Measure35\n",
      "./Measure36\n",
      "./Measure37\n",
      "./Measure38\n",
      "./Measure39\n",
      "./Measure40\n",
      "./Measure41\n",
      "./Measure42\n",
      "./Measure43\n",
      "./Measure44\n",
      "./Measure45\n",
      "./Measure46\n",
      "./Measure47\n",
      "./Measure48\n",
      "./Measure49\n",
      "./Measure50\n",
      "./Measure51\n",
      "./Measure52\n",
      "./Measure53\n",
      "./Measure54\n",
      "./Measure55\n",
      "./Measure56\n",
      "./Measure57\n",
      "./Measure58\n",
      "./Measure59\n",
      "./Measure60\n",
      "./Measure61\n",
      "./Measure62\n",
      "./Measure63\n",
      "./Measure64\n",
      "./Measure65\n",
      "./Measure66\n",
      "./Measure67\n",
      "./Measure68\n",
      "0.0001\n",
      "Epoch:  1\n",
      "Training MAE and MSE:  199.072461446 200.303940335\n",
      "Validate MAE and MSE:  211.052032471 211.22036499\n",
      "Epoch:  2\n",
      "Training MAE and MSE:  233.504528046 235.073116917\n",
      "Validate MAE and MSE:  262.692054749 262.782074559\n",
      "Epoch:  3\n",
      "Training MAE and MSE:  330.27474467 334.045819775\n",
      "Validate MAE and MSE:  403.96938324 404.461358368\n",
      "Epoch:  4\n",
      "Training MAE and MSE:  500.499491374 503.549409352\n",
      "Validate MAE and MSE:  563.19581604 565.022522745\n",
      "Epoch:  5\n",
      "Training MAE and MSE:  569.45988973 570.769258285\n",
      "Validate MAE and MSE:  560.007369995 561.664813076\n",
      "Epoch:  6\n",
      "Training MAE and MSE:  544.638819377 545.899776555\n",
      "Validate MAE and MSE:  533.121536255 534.403720688\n",
      "Test MAE and MSE:  550.057785034 550.438750819\n",
      "./Measure0\n",
      "./Measure1\n",
      "./Measure2\n",
      "./Measure3\n",
      "./Measure4\n",
      "./Measure5\n",
      "./Measure6\n",
      "./Measure7\n",
      "./Measure8\n",
      "./Measure9\n",
      "./Measure10\n",
      "./Measure11\n",
      "./Measure12\n",
      "./Measure13\n",
      "./Measure14\n",
      "./Measure15\n",
      "./Measure16\n",
      "./Measure17\n",
      "./Measure18\n",
      "./Measure19\n",
      "./Measure20\n",
      "./Measure21\n",
      "./Measure22\n",
      "./Measure23\n",
      "./Measure24\n",
      "./Measure25\n",
      "./Measure26\n",
      "./Measure27\n",
      "./Measure28\n",
      "./Measure29\n",
      "./Measure30\n",
      "./Measure31\n",
      "./Measure32\n",
      "./Measure33\n",
      "./Measure34\n",
      "./Measure35\n",
      "./Measure36\n",
      "./Measure37\n",
      "./Measure38\n",
      "./Measure39\n",
      "./Measure40\n",
      "./Measure41\n",
      "./Measure42\n",
      "./Measure43\n",
      "./Measure44\n",
      "./Measure45\n",
      "./Measure46\n",
      "./Measure47\n",
      "./Measure48\n",
      "./Measure49\n",
      "./Measure50\n",
      "./Measure51\n",
      "./Measure52\n",
      "./Measure53\n",
      "./Measure54\n",
      "./Measure55\n",
      "./Measure56\n",
      "./Measure57\n",
      "./Measure58\n",
      "./Measure59\n",
      "./Measure60\n",
      "./Measure61\n",
      "./Measure62\n",
      "./Measure63\n",
      "./Measure64\n",
      "./Measure65\n",
      "./Measure66\n",
      "./Measure67\n",
      "./Measure68\n",
      "./Measure69\n",
      "0.0001\n",
      "Epoch:  1\n",
      "Training MAE and MSE:  188.904425303 190.166242285\n",
      "Validate MAE and MSE:  199.080287933 199.264063329\n",
      "Epoch:  2\n",
      "Training MAE and MSE:  211.866556803 213.161147602\n",
      "Validate MAE and MSE:  229.679462433 229.804683609\n",
      "Epoch:  3\n",
      "Training MAE and MSE:  270.960000356 273.431989956\n",
      "Validate MAE and MSE:  321.805038452 321.993609516\n",
      "Epoch:  4\n",
      "Training MAE and MSE:  416.580591838 420.999539112\n",
      "Validate MAE and MSE:  499.712478638 501.231268196\n",
      "Epoch:  5\n",
      "Training MAE and MSE:  551.518447876 553.062836959\n",
      "Validate MAE and MSE:  570.624893188 572.873493035\n",
      "Epoch:  6\n",
      "Training MAE and MSE:  556.259841919 557.836718935\n",
      "Validate MAE and MSE:  540.511871338 542.268887702\n",
      "Test MAE and MSE:  559.121429443 559.620219356\n",
      "./Measure0\n",
      "./Measure1\n",
      "./Measure2\n",
      "./Measure3\n",
      "./Measure4\n",
      "./Measure5\n",
      "./Measure6\n",
      "./Measure7\n",
      "./Measure8\n",
      "./Measure9\n",
      "./Measure10\n",
      "./Measure11\n",
      "./Measure12\n",
      "./Measure13\n",
      "./Measure14\n",
      "./Measure15\n",
      "./Measure16\n",
      "./Measure17\n",
      "./Measure18\n",
      "./Measure19\n",
      "./Measure20\n",
      "./Measure21\n",
      "./Measure22\n",
      "./Measure23\n",
      "./Measure24\n",
      "./Measure25\n",
      "./Measure26\n",
      "./Measure27\n",
      "./Measure28\n",
      "./Measure29\n",
      "./Measure30\n",
      "./Measure31\n",
      "./Measure32\n",
      "./Measure33\n",
      "./Measure34\n",
      "./Measure35\n",
      "./Measure36\n",
      "./Measure37\n",
      "./Measure38\n",
      "./Measure39\n",
      "./Measure40\n",
      "./Measure41\n",
      "./Measure42\n",
      "./Measure43\n",
      "./Measure44\n",
      "./Measure45\n",
      "./Measure46\n",
      "./Measure47\n",
      "./Measure48\n",
      "./Measure49\n",
      "./Measure50\n",
      "./Measure51\n",
      "./Measure52\n",
      "./Measure53\n",
      "./Measure54\n",
      "./Measure55\n",
      "./Measure56\n",
      "./Measure57\n",
      "./Measure58\n",
      "./Measure59\n",
      "./Measure60\n",
      "./Measure61\n",
      "./Measure62\n",
      "./Measure63\n",
      "./Measure64\n",
      "./Measure65\n",
      "./Measure66\n",
      "./Measure67\n",
      "./Measure68\n",
      "./Measure69\n",
      "./Measure70\n",
      "0.0001\n",
      "Epoch:  1\n",
      "Training MAE and MSE:  184.938391368 186.174731597\n",
      "Validate MAE and MSE:  192.7317276 192.923101494\n",
      "Epoch:  2\n",
      "Training MAE and MSE:  201.704045614 202.940438314\n",
      "Validate MAE and MSE:  215.02130127 215.166814326\n",
      "Epoch:  3\n",
      "Training MAE and MSE:  246.440816243 248.500802914\n",
      "Validate MAE and MSE:  287.522819519 287.653158762\n",
      "Epoch:  4\n",
      "Training MAE and MSE:  373.978769938 378.560598626\n",
      "Validate MAE and MSE:  457.513069153 458.858410776\n",
      "Epoch:  5\n",
      "Training MAE and MSE:  527.920013428 529.95458002\n",
      "Validate MAE and MSE:  562.952529907 565.52837117\n",
      "Epoch:  6\n",
      "Training MAE and MSE:  552.686882019 554.446451729\n",
      "Validate MAE and MSE:  538.163665771 540.27199679\n",
      "Test MAE and MSE:  557.407501221 558.000051442\n"
     ]
    }
   ],
   "source": [
    "## INITIALISATION ##\n",
    "if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "    init = tf.initialize_all_variables()\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    \n",
    "## TENSOR BOARD ##\n",
    "if tensorBoard:\n",
    "    # Merge all summaries into a single operator\n",
    "    merged_summary_op = tf.summary.merge_all() \n",
    "      \n",
    "    for learningRate in [1e-4,1e-4,1e-4,1e-4]:#Learning rate for multiple search\n",
    "        ### Create a new folder for the measures ###\n",
    "        path = mainPath\n",
    "        i=0\n",
    "        while os.path.exists(path):\n",
    "            path = mainPath+str(i)\n",
    "            i=i+1\n",
    "            print(path)\n",
    "\n",
    "        os.makedirs(path)\n",
    "        os.makedirs(path+'/TensorBoard')\n",
    "        os.makedirs(path+'/SaveModel')\n",
    "        os.makedirs(path+'/Results')\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allocator_type = 'BFC'\n",
    "        with tf.Session(config = config) as sess:\n",
    "            print(learningRate)\n",
    "            sess.run(init)\n",
    "            tf.set_random_seed(1)\n",
    "\n",
    "            ### SAVE DATA ###\n",
    "            if saveVariable:\n",
    "                saver = tf.train.Saver()\n",
    "\n",
    "            ### TENSORBOARD ###\n",
    "            if tensorBoard:\n",
    "                # Folder where the data are saved\n",
    "                summary_writer = tf.summary.FileWriter(path+'/TensorBoard/', sess.graph)\n",
    "\n",
    "            if int(numImg*trainSize) < batchSize:#If there are less data than the batch size\n",
    "                nbBatch=1\n",
    "            else:\n",
    "                nbBatch=int((numImg*trainSize)/batchSize)\n",
    "\n",
    "            if int(numImg*valSize) < batchSize:#If there are less data than the batch size\n",
    "                nbBatchVal=1\n",
    "            else:\n",
    "                nbBatchVal=int((numImg*valSize)/batchSize)\n",
    "\n",
    "            # Init variables \n",
    "            trainMAE =  [0 for x in range(epochs)]\n",
    "            validateMAE =  [0 for x in range(epochs)]\n",
    "            trainMSE =  [0 for x in range(epochs)]\n",
    "            validateMSE =  [0 for x in range(epochs)]\n",
    "\n",
    "\n",
    "            for epoch in range(epochs):#Go through all the epochs\n",
    "                for batchNum in range(nbBatch):#Go through all the batches\n",
    "                    #print(batchNum)\n",
    "                    if batchSize==1:\n",
    "                        batch_xs, batch_ys = importImages(0, (numImg*trainSize))\n",
    "                    else:\n",
    "                        batch_xs, batch_ys = importImages((batchNum*batchSize), (batchNum*batchSize)+batchSize)\n",
    "\n",
    "                    sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys})\n",
    "                    mae, mse = compute_accuracy(batch_xs, batch_ys)\n",
    "                    trainMAE[epoch] = trainMAE[epoch]+mae\n",
    "                    trainMSE[epoch] = trainMSE[epoch]+(mse**2)\n",
    "\n",
    "                trainMAE[epoch]=trainMAE[epoch]/nbBatch\n",
    "                trainMSE[epoch]=np.sqrt(trainMSE[epoch]/nbBatch)\n",
    "                print('Epoch: ',epoch+1)       \n",
    "                print('Training MAE and MSE: ',trainMAE[epoch],trainMSE[epoch])\n",
    "\n",
    "                 ### TENSORBOARD ###\n",
    "                if tensorBoard:\n",
    "                    summary_str = sess.run(merged_summary_op, feed_dict={xs: batch_xs, ys: batch_ys})\n",
    "                    summary_writer.add_summary(summary_str, epoch)\n",
    "                ### SAVE DATA ###\n",
    "                if epoch%1==0:\n",
    "                    if saveVariable:\n",
    "                        saver.save(sess, path+'/SaveModel/cnnSave', global_step=epoch)\n",
    "\n",
    "\n",
    "\n",
    "                ##### Validation\n",
    "                idx1 = int((numImg*trainSize)) #start of the validationset\n",
    "                for batchNumVal in range(nbBatchVal):#Go through all the batches\n",
    "                    if batchSize==1:\n",
    "                        batchVal_xs, batchVal_ys = importImages(idx1, idx1+(numImg*valSize))\n",
    "                    else:\n",
    "                        batchVal_xs, batchVal_ys = importImages(idx1+(batchNumVal*batchSize), idx1+(batchNumVal*batchSize)+batchSize)\n",
    "\n",
    "                    mae, mse = compute_accuracy(batchVal_xs, batchVal_ys)\n",
    "                    validateMAE[epoch] = validateMAE[epoch]+mae\n",
    "                    validateMSE[epoch] = validateMSE[epoch]+(mse**2)\n",
    "\n",
    "                validateMAE[epoch]=validateMAE[epoch]/nbBatchVal\n",
    "                validateMSE[epoch]=np.sqrt(validateMSE[epoch]/nbBatchVal)\n",
    "                print('Validate MAE and MSE: ',validateMAE[epoch],validateMSE[epoch])\n",
    "\n",
    "\n",
    "\n",
    "            #########Test\n",
    "            if int(numImg*testSize) < batchSize:#If there are less data than the batch size\n",
    "                nbBatchTest=1\n",
    "            else:\n",
    "                nbBatchTest=int((numImg*testSize)/batchSize)\n",
    "\n",
    "            ##### Test\n",
    "            idx2 = int((numImg*trainSize)+(numImg*valSize)) #start of the validationset\n",
    "            testMAE=0\n",
    "            testMSE=0\n",
    "            for batchNumTest in range(nbBatchTest):#Go through all the batches\n",
    "                if batchSize==1:\n",
    "                    batchTest_xs, batchTest_ys = importImages(idx2, idx2+(numImg*testSize))\n",
    "                else:\n",
    "                    batchTest_xs, batchTest_ys = importImages(idx2+(batchNumTest*batchSize), idx2+(batchNumTest*batchSize)+batchSize)\n",
    "\n",
    "                mae, mse = compute_accuracy(batchTest_xs, batchTest_ys)\n",
    "                testMAE = testMAE+mae\n",
    "                testMSE = testMSE+(mse**2)\n",
    "\n",
    "            testMAE=testMAE/nbBatchTest\n",
    "            testMSE=np.sqrt(testMSE/nbBatchTest)\n",
    "            print('Test MAE and MSE: ',testMAE,testMSE)\n",
    "        #Save the MAE and MSE of each epoch\n",
    "        np.savetxt(path+\"/Results/EpochMAEMSE.csv\", np.transpose([trainMAE, trainMSE, validateMAE, validateMSE]), delimiter=\",\")\n",
    "        np.savetxt(path+\"/Results/TESTMAEMSE.csv\", np.transpose([testMAE, testMSE]), delimiter=\",\")\n",
    "        np.savetxt(path+\"/Results/PARAM.csv\", np.transpose([epoch, batchSize, learningRate, numImg, shuffleData, trainSize, valSize, testSize]), delimiter=\",\")\n",
    "\n",
    "    # Plot MAE and MSE during training\n",
    "\n",
    "        plt.subplot(121) # MAE\n",
    "        plt.plot(np.linspace(1,epochs,epochs),trainMAE,'b',label='train')\n",
    "        plt.plot(np.linspace(1,epochs,epochs),validateMAE,'r',label='validation')\n",
    "        plt.xlabel('Training time [epoch]')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend(['train','validation'])\n",
    "        plt.title('Mean Absolute Error')\n",
    "\n",
    "        plt.subplot(122) # MSE\n",
    "        plt.plot(np.linspace(1,epochs,epochs),trainMSE,'b',label='train')\n",
    "        plt.plot(np.linspace(1,epochs,epochs),validateMSE,'r',label='validation')\n",
    "        plt.xlabel('Training time [epoch]')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.legend(['train','validation'])\n",
    "        plt.title('Mean Square Error')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path+'/Results/graphs.png')\n",
    "        #plt.show()\n",
    "        plt.gcf().clear()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "np.savetxt(path+\"/Results/EpochMAEMSE.csv\", np.transpose([trainMAE, trainMSE, validateMAE, validateMSE]), delimiter=\",\")\n",
    "np.savetxt(path+\"/Results/TESTMAEMSE.csv\", np.transpose([testMAE, testMSE]), delimiter=\",\")\n",
    "np.savetxt(path+\"/Results/PARAM.csv\", np.transpose([epoch, batchSize, learningRate, numImg, shuffleData, trainSize, valSize, testSize]), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plot MAE and MSE during training\n",
    "\n",
    "plt.subplot(121) # MAE\n",
    "plt.plot(np.linspace(1,epochs,epochs),trainMAE,'b',label='train')\n",
    "plt.plot(np.linspace(1,epochs,epochs),validateMAE,'r',label='validation')\n",
    "plt.xlabel('Training time [epoch]')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend(['train','validation'])\n",
    "plt.title('Mean Absolute Error')\n",
    "\n",
    "plt.subplot(122) # MSE\n",
    "plt.plot(np.linspace(1,epochs,epochs),trainMSE,'b',label='train')\n",
    "plt.plot(np.linspace(1,epochs,epochs),validateMSE,'r',label='validation')\n",
    "plt.xlabel('Training time [epoch]')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(['train','validation'])\n",
    "plt.title('Mean Square Error')\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+'/Results/graphs.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TEST SESSION ###\n",
    "init = tf.global_variables_initializer()\n",
    "savePath =  path+'/Results/'\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for im in range(100):\n",
    "    #imNumber = 125\n",
    "        imgsTest,labelsTest = importImages(im+1,im+2)\n",
    "        result = evaluateNetwork(imgsTest)\n",
    "        np.savetxt(savePath+str(im)+\"feedforward.csv\", result, delimiter=\",\")\n",
    "        np.savetxt(savePath+str(im)+\"ffGT.csv\", labelsTest, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TEST SESSION ###\n",
    "init = tf.global_variables_initializer()\n",
    "savePath = path+'/Results/'\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for im in range(len(batch_xs)):\n",
    "    #imNumber = 125\n",
    "        imgsTrain=batch_xs[im]\n",
    "        labelsTrain = batch_ys[im]\n",
    "        result = evaluateNetwork(imgsTrain)\n",
    "        np.savetxt(savePath+str(im)+\"feedforward.csv\", result, delimiter=\",\")\n",
    "        np.savetxt(savePath+str(im)+\"ffGT.csv\", labelsTrain, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env tensorflowGPU",
   "language": "python",
   "name": "tensorflowgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
