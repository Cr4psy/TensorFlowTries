{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network applied for crowd counting\n",
    "#### Deep learning : DD2424\n",
    "## Anthony Clerc & Valur Sigurbjorn Palmarsson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.misc, os, csv, random, math\n",
    "from subprocess import check_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainImagePath='./Dataset/imgpp/'\n",
    "labelsPath = './Dataset/GT/'\n",
    "\n",
    "mainPath='./Measure' #Path where the different measure are saved\n",
    "\n",
    "epochs=6\n",
    "batchSize=10\n",
    "learningRate=1e-4\n",
    "numImg = 200 #Total number of images\n",
    "shuffleData = False #Shuffle data\n",
    "\n",
    "# Size of each subset as precentage of entire dataset\n",
    "trainSize=0.6\n",
    "valSize=0.2\n",
    "testSize=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the images & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def importImages(nbImgStart, nbImgEnd):\n",
    "    #Extract all the file available in the folder\n",
    "    fileNames = os.listdir(trainImagePath)\n",
    "    #Select a subset of files\n",
    "    fileNames = fileNames[nbImgStart:nbImgEnd]\n",
    "    \n",
    "    #Extract all the image flatten and write them in an array\n",
    "    imgs=[] # Init image list\n",
    "    labels=[] # Init labels list\n",
    "    for fileName in fileNames: \n",
    "        # Load images\n",
    "        img = scipy.misc.imread(trainImagePath+fileName, False,'RGB')     \n",
    "        img = img.ravel()#Flatten the image\n",
    "        imgs.append(img)#Append to one common list\n",
    "\n",
    "        # Load labels\n",
    "        labelName = fileName.partition('pp')[0]+'GT.csv'#Use the same image number\n",
    "        with open(labelsPath+labelName, 'r') as f:#Read the file\n",
    "            reader = csv.reader(f)\n",
    "            label= np.asarray(list(reader), dtype=float)#Extract the value in an array\n",
    "            label = label.ravel()#flatten\n",
    "            labels.append(label)#Add to the list\n",
    "\n",
    "    #Convert to a matrix\n",
    "    imgs=np.matrix(imgs, dtype=float)/255\n",
    "    labels = np.matrix(labels, dtype=float)\n",
    "    \n",
    "    \n",
    "    # Shuffle images to a random order, (shuffles imgs and labels in the same random order)\n",
    "    if shuffleData:\n",
    "        combined = list(zip(imgs, labels))\n",
    "        random.shuffle(combined)\n",
    "        imgs[:], labels[:] = zip(*combined)\n",
    "\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS ##\n",
    "tensorBoard=True       #Save a summary for tensorboard?\n",
    "saveVariable=False     #Save the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "#Provide a feedforward path\n",
    "def evaluateNetwork(v_xs):\n",
    "    global prediction\n",
    "    y_pre = sess.run( tf.cast(prediction, tf.float32), feed_dict={xs: v_xs}) # Forward pass using v_xs as input to network\n",
    "    result = np.matrix(y_pre, dtype=float)\n",
    "    return result\n",
    "\n",
    "# Calculate mean absolute error (MAE) and mean square error (MSE) for evaluation\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_label = tf.reshape(v_ys, [-1, 43, 43, 1])\n",
    "    y_label = tf.cast(y_label, tf.float32)\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs})\n",
    "    y_pre = tf.cast(y_pre, tf.float32)#Force float32\n",
    "\n",
    "    diff = tf.subtract(tf.reduce_sum(y_label,[1,2]),tf.reduce_sum(y_pre,[1,2]))\n",
    "    accuracyMAE = tf.cast(tf.reduce_mean(tf.abs(diff)),tf.float32)\n",
    "    accuracyMSE = tf.cast(tf.sqrt(tf.reduce_mean(tf.square(diff))),tf.float32)\n",
    "    MAE = sess.run(accuracyMAE, feed_dict={xs: v_xs, ys: v_ys})\n",
    "    MSE = sess.run(accuracyMSE, feed_dict={xs: v_xs, ys: v_ys})\n",
    "    \n",
    "    return MAE,MSE\n",
    "\n",
    "def weight_variable(shape, nameIn):\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=0.1)   \n",
    "    #initial = tf.random_normal(shape=shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=nameIn)\n",
    "\n",
    "def bias_variable(shape, nameIn):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=nameIn)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def max_pool_3x3(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,3,3,1], strides=[1,3,3,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Architecture\n",
    "\n",
    "## Input layer ##\n",
    "with tf.name_scope(\"Input\") as scope:\n",
    "    # define placeholder for inputs to network\n",
    "    xs = tf.placeholder(tf.float32, [None, 256*256*3] #Flatten image imput\n",
    "    ys = tf.placeholder(tf.float32, [None,43*43])     #Flatten density map input\n",
    "    #Reshape the images to apply convolution\n",
    "    x_image = tf.reshape(xs, [-1, 256, 256, 3])#[batch, in_height, in_width, in_channels].\n",
    "    y_label = tf.reshape(ys, [-1, 43, 43, 1])\n",
    "\n",
    "    \n",
    "## conv1 layer ##\n",
    "## maxpooling 2x2 ##\n",
    "#20x (7x7)\n",
    "patch=7\n",
    "sizeIn=3\n",
    "sizeOut=20\n",
    "with tf.name_scope(\"Conv1\") as scope:\n",
    "    W_conv1 = weight_variable([patch,patch, sizeIn,sizeOut], \"W_conv1\")\n",
    "    b_conv1 = bias_variable([sizeOut], \"b_conv1\")\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1) #Maxpooling 2x2, output size= inout size/2\n",
    "    # Add summary ops to collect data\n",
    "    w_h_conv1 = tf.summary.histogram(\"weightsConv1\", W_conv1)\n",
    "    b_h_conv1 = tf.summary.histogram(\"biasesConv1\", b_conv1)\n",
    "    \n",
    "## conv2 layer ##\n",
    "## maxpooling 3x3 ##\n",
    "#40x (5x5)\n",
    "patch=5\n",
    "sizeIn=sizeOut #From previous ConvNet\n",
    "sizeOut=40\n",
    "with tf.name_scope(\"Conv2\") as scope:\n",
    "    W_conv2 = weight_variable([patch,patch, sizeIn,sizeOut],\"W_conv2\")\n",
    "    b_conv2 = bias_variable([sizeOut],\"b_conv2\")\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_3x3(h_conv2) #Maxpooling 3x3, output size= inout size/3\n",
    "    # Add summary ops to collect data\n",
    "    w_h_conv2 = tf.summary.histogram(\"weightsConv2\", W_conv2)\n",
    "    b_h_conv2 = tf.summary.histogram(\"biasesConv2\", b_conv2)\n",
    "    \n",
    "## conv3 layer ##\n",
    "#20x (5x5)\n",
    "patch=5\n",
    "sizeIn=sizeOut #From previous ConvNet\n",
    "sizeOut=20\n",
    "with tf.name_scope(\"Conv3\") as scope:\n",
    "    W_conv3 = weight_variable([patch,patch, sizeIn,sizeOut],\"W_conv3\")\n",
    "    b_conv3 = bias_variable([sizeOut],\"b_conv3\")\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "    # Add summary ops to collect data\n",
    "    w_h_conv3 = tf.summary.histogram(\"weightsConv3\", W_conv3)\n",
    "    b_h_conv3 = tf.summary.histogram(\"biasesConv3\", b_conv3)\n",
    "    \n",
    "## conv4 layer ##\n",
    "#10x (5x5)\n",
    "patch=5\n",
    "sizeIn=sizeOut #From previous ConvNet\n",
    "sizeOut=10\n",
    "with tf.name_scope(\"Conv4\") as scope:\n",
    "    W_conv4 = weight_variable([patch,patch, sizeIn,sizeOut],\"W_conv4\")\n",
    "    b_conv4 = bias_variable([sizeOut],\"b_conv4\")\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4) + b_conv4)\n",
    "    # Add summary ops to collect data\n",
    "    w_h_conv4 = tf.summary.histogram(\"weightsConv4\", W_conv4)\n",
    "    b_h_conv4 = tf.summary.histogram(\"biasesConv4\", b_conv4)\n",
    "    \n",
    "## conv5 layer ##\n",
    "#1x (1x1)\n",
    "patch=1\n",
    "sizeIn=sizeOut #From previous ConvNet\n",
    "sizeOut=1\n",
    "with tf.name_scope(\"Conv5\") as scope:\n",
    "    W_conv5 = weight_variable([patch,patch, sizeIn,sizeOut], \"W_conv5\")\n",
    "    b_conv5 = bias_variable([sizeOut], \"b_conv5\")\n",
    "    h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5) + b_conv5)\n",
    "    # Add summary ops to collect data\n",
    "    w_h_conv5 = tf.summary.histogram(\"weightsConv5\", W_conv5)\n",
    "    b_h_conv5 = tf.summary.histogram(\"biasesConv5\", b_conv5)\n",
    "    \n",
    "   \n",
    " ## Prediction ##\n",
    "with tf.name_scope(\"prediction\") as scope:\n",
    "    prediction = h_conv5\n",
    "    img_prediction = tf.summary.image(\"densitymap\", prediction)\n",
    "\n",
    "\n",
    "## Loss function ##\n",
    "# the error between prediction and real data\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    loss = tf.reduce_mean(tf.square(tf.reduce_max(tf.abs(tf.subtract(prediction,y_label)),[1,2])))\n",
    "    #loss = tf.reduce_mean(tf.square(tf.reduce_max(tf.subtract(prediction,y_label),[1,2])))\n",
    "    #loss = tf.reshape(tf.reduce_sum(tf.square((tf.subtract(prediction, y_label))),0),[-1])#Euclidean distance\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    train_step = tf.train.AdamOptimizer(learningRate).minimize(loss)\n",
    "    #train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## INITIALISATION ##\n",
    "if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "    init = tf.initialize_all_variables()\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    \n",
    "## TENSOR BOARD ##\n",
    "if tensorBoard:\n",
    "    # Merge all summaries into a single operator\n",
    "    merged_summary_op = tf.summary.merge_all() \n",
    "      \n",
    "    for learningRate in [1e-4,1e-4,1e-4,1e-4]:#Learning rate for multiple search\n",
    "        \n",
    "        ### Create a new folder for the measures ###\n",
    "        path = mainPath\n",
    "        i=0\n",
    "        while os.path.exists(path):\n",
    "            path = mainPath+str(i)\n",
    "            i=i+1\n",
    "            print(path)\n",
    "        os.makedirs(path)\n",
    "        os.makedirs(path+'/TensorBoard')\n",
    "        os.makedirs(path+'/SaveModel')\n",
    "        os.makedirs(path+'/Results')\n",
    "\n",
    "        \n",
    "        config = tf.ConfigProto()#Param for memory allocation\n",
    "        config.gpu_options.allocator_type = 'BFC'\n",
    "        with tf.Session(config = config) as sess:\n",
    "            print(learningRate)\n",
    "            sess.run(init)\n",
    "\n",
    "            ### SAVE DATA ###\n",
    "            if saveVariable:\n",
    "                saver = tf.train.Saver()\n",
    "\n",
    "            ### TENSORBOARD ###\n",
    "            if tensorBoard:\n",
    "                # Folder where the data are saved\n",
    "                summary_writer = tf.summary.FileWriter(path+'/TensorBoard/', sess.graph)\n",
    "\n",
    "                \n",
    "            ## Define the number of batch ##\n",
    "            if int(numImg*trainSize) < batchSize:#If there are less data than the batch size\n",
    "                nbBatch=1\n",
    "            else:\n",
    "                nbBatch=int((numImg*trainSize)/batchSize)\n",
    "\n",
    "            if int(numImg*valSize) < batchSize:#If there are less data than the batch size\n",
    "                nbBatchVal=1\n",
    "            else:\n",
    "                nbBatchVal=int((numImg*valSize)/batchSize)\n",
    "\n",
    "            # Init variables \n",
    "            trainMAE =  [0 for x in range(epochs)]\n",
    "            validateMAE =  [0 for x in range(epochs)]\n",
    "            trainMSE =  [0 for x in range(epochs)]\n",
    "            validateMSE =  [0 for x in range(epochs)]\n",
    "\n",
    "            ### TRAINING ###\n",
    "            for epoch in range(epochs):#Go through all the epochs\n",
    "                for batchNum in range(nbBatch):#Go through all the batches\n",
    "                    if batchSize==1:\n",
    "                        batch_xs, batch_ys = importImages(0, (numImg*trainSize))\n",
    "                    else:\n",
    "                        batch_xs, batch_ys = importImages((batchNum*batchSize), (batchNum*batchSize)+batchSize)\n",
    "\n",
    "                    sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys})\n",
    "                    mae, mse = compute_accuracy(batch_xs, batch_ys)\n",
    "                    trainMAE[epoch] = trainMAE[epoch]+mae\n",
    "                    trainMSE[epoch] = trainMSE[epoch]+(mse**2)\n",
    "\n",
    "                trainMAE[epoch]=trainMAE[epoch]/nbBatch\n",
    "                trainMSE[epoch]=np.sqrt(trainMSE[epoch]/nbBatch)\n",
    "                print('Epoch: ',epoch+1)       \n",
    "                print('Training MAE and MSE: ',trainMAE[epoch],trainMSE[epoch])\n",
    "\n",
    "                 ### TENSORBOARD ###\n",
    "                if tensorBoard:\n",
    "                    summary_str = sess.run(merged_summary_op, feed_dict={xs: batch_xs, ys: batch_ys})\n",
    "                    summary_writer.add_summary(summary_str, epoch)\n",
    "                ### SAVE DATA ###\n",
    "                if epoch%1==0:\n",
    "                    if saveVariable:\n",
    "                        saver.save(sess, path+'/SaveModel/cnnSave', global_step=epoch)\n",
    "\n",
    "\n",
    "\n",
    "                ##### Validation\n",
    "                #The computation is splitted in batches due to the limit of the computers\n",
    "                idx1 = int((numImg*trainSize)) #start of the validationset\n",
    "                for batchNumVal in range(nbBatchVal):#Go through all the batches\n",
    "                    if batchSize==1:\n",
    "                        batchVal_xs, batchVal_ys = importImages(idx1, idx1+(numImg*valSize))\n",
    "                    else:\n",
    "                        batchVal_xs, batchVal_ys = importImages(idx1+(batchNumVal*batchSize), idx1+(batchNumVal*batchSize)+batchSize)\n",
    "\n",
    "                    mae, mse = compute_accuracy(batchVal_xs, batchVal_ys)\n",
    "                    validateMAE[epoch] = validateMAE[epoch]+mae\n",
    "                    validateMSE[epoch] = validateMSE[epoch]+(mse**2)\n",
    "\n",
    "                validateMAE[epoch]=validateMAE[epoch]/nbBatchVal\n",
    "                validateMSE[epoch]=np.sqrt(validateMSE[epoch]/nbBatchVal)\n",
    "                print('Validate MAE and MSE: ',validateMAE[epoch],validateMSE[epoch])\n",
    "\n",
    "\n",
    "\n",
    "            #########Test\n",
    "            if int(numImg*testSize) < batchSize:#If there are less data than the batch size\n",
    "                nbBatchTest=1\n",
    "            else:\n",
    "                nbBatchTest=int((numImg*testSize)/batchSize)\n",
    "\n",
    "            ##### Test\n",
    "            idx2 = int((numImg*trainSize)+(numImg*valSize)) #start of the validationset\n",
    "            testMAE=0\n",
    "            testMSE=0\n",
    "            for batchNumTest in range(nbBatchTest):#Go through all the batches\n",
    "                if batchSize==1:\n",
    "                    batchTest_xs, batchTest_ys = importImages(idx2, idx2+(numImg*testSize))\n",
    "                else:\n",
    "                    batchTest_xs, batchTest_ys = importImages(idx2+(batchNumTest*batchSize), idx2+(batchNumTest*batchSize)+batchSize)\n",
    "\n",
    "                mae, mse = compute_accuracy(batchTest_xs, batchTest_ys)\n",
    "                testMAE = testMAE+mae\n",
    "                testMSE = testMSE+(mse**2)\n",
    "\n",
    "            testMAE=testMAE/nbBatchTest\n",
    "            testMSE=np.sqrt(testMSE/nbBatchTest)\n",
    "            print('Test MAE and MSE: ',testMAE,testMSE)\n",
    "            \n",
    "        #Save all the different measure in a csv file and also the parameters\n",
    "        np.savetxt(path+\"/Results/EpochMAEMSE.csv\", np.transpose([trainMAE, trainMSE, validateMAE, validateMSE]), delimiter=\",\")\n",
    "        np.savetxt(path+\"/Results/TESTMAEMSE.csv\", np.transpose([testMAE, testMSE]), delimiter=\",\")\n",
    "        np.savetxt(path+\"/Results/PARAM.csv\", np.transpose([epoch, batchSize, learningRate, numImg, shuffleData, trainSize, valSize, testSize]), delimiter=\",\")\n",
    "\n",
    "        # Plot MAE and MSE of the different epoch\n",
    "        plt.subplot(121) # MAE\n",
    "        plt.plot(np.linspace(1,epochs,epochs),trainMAE,'b',label='train')\n",
    "        plt.plot(np.linspace(1,epochs,epochs),validateMAE,'r',label='validation')\n",
    "        plt.xlabel('Training time [epoch]')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend(['train','validation'])\n",
    "        plt.title('Mean Absolute Error')\n",
    "\n",
    "        plt.subplot(122) # MSE\n",
    "        plt.plot(np.linspace(1,epochs,epochs),trainMSE,'b',label='train')\n",
    "        plt.plot(np.linspace(1,epochs,epochs),validateMSE,'r',label='validation')\n",
    "        plt.xlabel('Training time [epoch]')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.legend(['train','validation'])\n",
    "        plt.title('Mean Square Error')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path+'/Results/graphs.png')\n",
    "        #plt.show()\n",
    "        plt.gcf().clear()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forwrd with new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TEST SESSION ###\n",
    "init = tf.global_variables_initializer()\n",
    "savePath = path+'/Results/'\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for im in range(len(batch_xs)):\n",
    "    #imNumber = 125\n",
    "        imgsTrain=batch_xs[im]\n",
    "        labelsTrain = batch_ys[im]\n",
    "        result = evaluateNetwork(imgsTrain)\n",
    "        np.savetxt(savePath+str(im)+\"feedforward.csv\", result, delimiter=\",\")\n",
    "        np.savetxt(savePath+str(im)+\"ffGT.csv\", labelsTrain, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env tensorflowGPU",
   "language": "python",
   "name": "tensorflowgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
